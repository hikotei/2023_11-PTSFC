{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsforecast.models import MSTL\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "\n",
    "os.chdir(\"C:/2023_11-PTSFC\")\n",
    "import model_train as model_train\n",
    "import data_prepro as data_prepro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"1\"  # Replace \"4\" with the desired number of cores\n",
    "\n",
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "fcast_hor = [36, 40, 44, 60, 64, 68] # in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 80136 entries, 2014-12-31 23:00:00+00:00 to 2024-02-21 22:00:00+00:00\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   timestamp_CET  80136 non-null  datetime64[ns, CET]\n",
      " 1   gesamt         80136 non-null  float64            \n",
      "dtypes: datetime64[ns, CET](1), float64(1)\n",
      "memory usage: 1.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = \n",
    "# get data\n",
    "# df_energy = data_prepro.get_energy_data_today(to_date=t_wednesday.strftime('%Y%m%d'))\n",
    "\n",
    "# Read data from file with specified data types\n",
    "df_energy = pd.read_csv(\"data/2015-01-01_2024-02-21_energy.csv\", index_col=0, parse_dates=[0])\n",
    "df_energy['timestamp_CET'] = pd.to_datetime(df_energy['timestamp_CET'], utc=True).dt.tz_convert('CET')\n",
    "print(df_energy.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_energy, start_date):\n",
    "    \n",
    "    df_energy_small = df_energy.loc[(df_energy['timestamp_CET'] > start_date)].copy()\n",
    "    \n",
    "    df_energy_dummy = data_prepro.create_dummy_df(df_energy_small, hour_method='seasonal', holiday_method='separate')\n",
    "    df_energy_fturs = data_prepro.create_features_df(df_energy_small, holiday_method='separate')\n",
    "\n",
    "    X_train_fturs = df_energy_fturs.drop(['gesamt', 'timestamp_CET'], axis=1)\n",
    "    y_train_fturs = df_energy_fturs['gesamt']\n",
    "\n",
    "    X_train_dummy = df_energy_dummy.drop(['gesamt', 'timestamp_CET'], axis=1)\n",
    "    y_train_dummy = df_energy_dummy['gesamt']\n",
    "    \n",
    "    return X_train_dummy, y_train_dummy, X_train_fturs, y_train_fturs\n",
    "\n",
    "def generate_param_grids(params):\n",
    "    \n",
    "        param_values = list(itertools.product(*params.values()))\n",
    "        param_names = list(params.keys())\n",
    "\n",
    "        param_grids = []\n",
    "\n",
    "        for values in param_values:\n",
    "            param_dict = dict(zip(param_names, values))\n",
    "            param_grids.append(param_dict)\n",
    "\n",
    "        return param_grids\n",
    "\n",
    "lgbm_params = {\n",
    "    'max_depth': [4, 10],\n",
    "    'num_leaves': [5, 15, 20],\n",
    "    'learning_rate': [0.1, 0.3],\n",
    "    'n_estimators': [100, 300],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': ['reg:quantileerror'],\n",
    "    'eval_metric': ['quantile'],\n",
    "    'booster': ['gbtree'],\n",
    "    'max_depth': [4, 10],\n",
    "    'learning_rate': [0.1, 0.3],\n",
    "    'n_estimators': [100, 200],\n",
    "}\n",
    "\n",
    "all_lgbm_params = generate_param_grids(lgbm_params)\n",
    "all_xgb_params = generate_param_grids(xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator QuantileRegressor from version 1.3.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator DummyRegressor from version 1.3.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.3.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator GradientBoostingRegressor from version 1.3.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "# import trained models\n",
    "path = f\"custom_models\"\n",
    "\n",
    "def load_pickled_object(path, filename):\n",
    "    with open(f\"{path}/{filename}\", \"rb\") as handle:\n",
    "        return pickle.load(handle)\n",
    "    \n",
    "quant_reg_models = {}\n",
    "quant_reg_models['quant_reg_2015_sk'] = load_pickled_object(path, \"20150101_20231031_all_models_quant_reg_sk.pickle\")\n",
    "quant_reg_models['quant_reg_2018_sk'] = load_pickled_object(path, \"20180101_20231031_all_models_quant_reg_sk.pickle\")\n",
    "quant_reg_models['quant_reg_2015_sm'] = load_pickled_object(path, \"20150101_20231031_all_models_quant_reg_sm.pickle\")\n",
    "quant_reg_models['quant_reg_2018_sm'] = load_pickled_object(path, \"20180101_20231031_all_models_quant_reg_sm.pickle\")\n",
    "\n",
    "grad_boost_models = {}\n",
    "grad_boost_models['grad_boost_2015_fturs'] = load_pickled_object(path, \"20150101_20231031_all_models_grad_boost_fturs.pickle\")\n",
    "grad_boost_models['grad_boost_2018_fturs'] = load_pickled_object(path, \"20180101_20231031_all_models_grad_boost_fturs.pickle\")\n",
    "grad_boost_models['grad_boost_2015_dummy'] = load_pickled_object(path, \"20150101_20231031_all_models_grad_boost_dummy.pickle\")\n",
    "grad_boost_models['grad_boost_2018_dummy'] = load_pickled_object(path, \"20180101_20231031_all_models_grad_boost_dummy.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "Forecasting for week starting from 2023-11-15 ...\n",
      "Submission timestamps = 2023-11-17 12:00:00+01:00 to 2023-11-18 20:00:00+01:00\n",
      "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method = bench_pm_2weeks\n",
      "method = bench_same_month\n",
      "method = bench_pm_1month\n",
      "method = mstl_1\n",
      "method = mstl_0.5\n",
      "method = quant_reg_2015_sk\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- hour_1\n- hour_10\n- hour_11\n- hour_12\n- hour_13\n- ...\nFeature names seen at fit time, yet now missing:\n- hour_10_summer\n- hour_10_winter\n- hour_11_summer\n- hour_11_winter\n- hour_12_summer\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 164\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Prediction for Quantile Regression\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(all_models_quant_reg\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m--> 164\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_fcast_dummy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp_CET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     df_quant_reg_direct_fcast[name] \u001b[38;5;241m=\u001b[39m pred\n\u001b[0;32m    167\u001b[0m dict_weekly_fcasts[method] \u001b[38;5;241m=\u001b[39m df_quant_reg_direct_fcast\n",
      "File \u001b[1;32mc:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32mc:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    517\u001b[0m ):\n\u001b[0;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    503\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[1;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- hour_1\n- hour_10\n- hour_11\n- hour_12\n- hour_13\n- ...\nFeature names seen at fit time, yet now missing:\n- hour_10_summer\n- hour_10_winter\n- hour_11_summer\n- hour_11_winter\n- hour_12_summer\n- ...\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "folder_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = os.path.join(cwd, folder_name)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "os.chdir(directory)\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = pd.Timestamp('2023-11-15')\n",
    "end_date = pd.Timestamp('2024-02-14')\n",
    "\n",
    "# Generate a list of weekly dates in UTC\n",
    "fcast_dates_cet = pd.date_range(start=start_date, end=end_date, freq='W-WED').tz_localize('CET').strftime('%Y-%m-%d').tolist()\n",
    "dict_all_fcasts = {}\n",
    "dict_all_evals = {}\n",
    "\n",
    "# Iterate over the forecast dates\n",
    "for fcast_date in fcast_dates_cet[-2:]:\n",
    "\n",
    "    print('= '*30)\n",
    "    print(f\"Forecasting for week starting from {fcast_date} ...\")\n",
    "    \n",
    "    dict_weekly_fcasts = {}\n",
    "    dict_weekly_models = {}\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # generate prediction timestamps based on t0 = following thursday 00:00\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    # Calculate the Thursday and Wednesday of the week\n",
    "    t_wednesday = pd.Timestamp(fcast_date).replace(hour=0, minute=0, second=0, microsecond=0).tz_localize('CET')\n",
    "    t_thursday = t_wednesday + pd.Timedelta(days=1)\n",
    "\n",
    "    # Generate required submission timestamps\n",
    "    subm_timestamps = [(t_thursday + pd.Timedelta(hours=fcast)) for fcast in fcast_hor]\n",
    "    print(f\"Submission timestamps = {subm_timestamps[0]} to {subm_timestamps[-1]}\")\n",
    "    \n",
    "    # Create df with Information at fcast start date\n",
    "    t_thursday_str = t_thursday.strftime('%Y-%m-%d')\n",
    "    df_energy_current = df_energy.loc[df_energy['timestamp_CET'] <= t_thursday_str].copy()\n",
    "    print('= '*30)\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # Simple Benchmark\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    # create fcast index for next 68 hours\n",
    "    fcast_timestamp_CET = pd.date_range(start=t_thursday, periods=68+1, freq='H')\n",
    "    fcast_timestamp_UTC = fcast_timestamp_CET.tz_convert('UTC')\n",
    "    \n",
    "    # create new dataframe with relevant info for benchmark\n",
    "    df_energy_benchmark = df_energy_current.copy()\n",
    "    df_energy_benchmark[\"month\"] = df_energy_benchmark['timestamp_CET'].dt.month\n",
    "    df_energy_benchmark[\"weekday\"] = df_energy_benchmark['timestamp_CET'].dt.weekday # Monday=0, Sunday=6\n",
    "    df_energy_benchmark[\"weeknum\"] = df_energy_benchmark['timestamp_CET'].dt.isocalendar().week\n",
    "\n",
    "    last_t = 150 # if there are more matches only take the most recent\n",
    "    pred_baseline = np.zeros((3,len(fcast_timestamp_CET),5)) # 3 condition types, 5 quantiles\n",
    "\n",
    "    for i,d in enumerate(fcast_timestamp_CET):\n",
    "            \n",
    "        weekday = d.weekday()\n",
    "        hour = d.hour\n",
    "        weeknum = d.week\n",
    "        \n",
    "        # basic condition that the weekday and hour match\n",
    "        basic_cond = (df_energy_benchmark.weekday == weekday) & (df_energy_benchmark.index.time == d.time())\n",
    "        \n",
    "        # AND the weeknum is within +/- 2 weeks of the target\n",
    "        cond1 = (df_energy_benchmark['weeknum'].between(weeknum-2, weeknum+2)) \n",
    "        # AND the month also matches\n",
    "        cond2 = (df_energy_benchmark.index.month == d.month)\n",
    "        # AND the month is within +/- 1 months of the target\n",
    "        cond3 = (df_energy_benchmark['month'].between(d.month-1, d.month+1))\n",
    "\n",
    "        cond_list = [cond1, cond2, cond3]\n",
    "\n",
    "        for cond_idx, cond in enumerate(cond_list):\n",
    "            cond = basic_cond & cond\n",
    "            match_df = df_energy_benchmark[cond]\n",
    "            # print(f\"condition {cond_idx} ... {len(match_df)} matches found\")\n",
    "            pred_baseline[cond_idx, i, :] = np.quantile(match_df.iloc[-last_t:][\"gesamt\"], q=quantiles) # method='linear'\n",
    "\n",
    "    methods = ['bench_pm_2weeks', 'bench_same_month', 'bench_pm_1month']\n",
    "    for m_idx, method in enumerate(methods):\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "        # create empty df\n",
    "        df_benchmark = pd.DataFrame(index=fcast_timestamp_UTC, columns=[f\"q {q:.3f}\" for q in quantiles])\n",
    "        df_benchmark.loc[:,:] = pred_baseline[m_idx,:,:]\n",
    "\n",
    "        # make sure all cols are float\n",
    "        df_benchmark = df_benchmark.astype(float)\n",
    "        # add CET col\n",
    "        df_benchmark['timestamp_CET'] = df_benchmark.index.tz_convert('CET')\n",
    "        # reorder cols\n",
    "        df_benchmark = df_benchmark[['timestamp_CET', 'q 0.025', 'q 0.250', 'q 0.500', 'q 0.750', 'q 0.975']]\n",
    "        # save to dict\n",
    "        dict_weekly_fcasts[method] = df_benchmark\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # MSTL\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    mstl_train_horizon = 0.5 # in years\n",
    "\n",
    "    for mstl_train_horizon in [1, 0.5]:\n",
    "        \n",
    "        method = f\"mstl_{mstl_train_horizon}\"\n",
    "        print(f\"method = {method}\")\n",
    "\n",
    "        df_mstl_train = df_energy_current.iloc[-int(mstl_train_horizon * 365 * 24):].copy()\n",
    "        mstl_model = MSTL(season_length=[24, 24 * 7]).fit(df_mstl_train[\"gesamt\"])\n",
    "\n",
    "        n_steps = df_benchmark.shape[0]\n",
    "\n",
    "        y_hat_dict = mstl_model.predict(h=n_steps, level=[50, 95])\n",
    "        y_hat_df = pd.DataFrame(y_hat_dict)\n",
    "        y_hat_df[\"timestamp_CET\"] = pd.date_range(start=t_thursday, periods=len(y_hat_df), freq=\"H\")\n",
    "\n",
    "        # rename columns\n",
    "        y_hat_df = y_hat_df.rename(\n",
    "            columns={\n",
    "                \"mean\": \"q 0.500\",\n",
    "                \"lo-50\": \"q 0.250\",\n",
    "                \"hi-50\": \"q 0.750\",\n",
    "                \"lo-95\": \"q 0.025\",\n",
    "                \"hi-95\": \"q 0.975\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # rearrange cols\n",
    "        y_hat_df = y_hat_df[[\"timestamp_CET\", \"q 0.025\", \"q 0.250\", \"q 0.500\", \"q 0.750\", \"q 0.975\"]]\n",
    "\n",
    "        df_mstl_fcast = y_hat_df\n",
    "        df_mstl_fcast.index = fcast_timestamp_UTC\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_mstl_fcast\n",
    "    \n",
    "    # = = = = = = = = = = = = = \n",
    "    # Quantile Reg\n",
    "    # = = = = = = = = = = = = = \n",
    "        \n",
    "    # create df with fcast timestamps as INPUT for model\n",
    "    df_temp = pd.DataFrame(index=fcast_timestamp_UTC)\n",
    "    df_temp['timestamp_CET'] = fcast_timestamp_CET\n",
    "    df_fcast_dummy = data_prepro.create_dummy_df(df_temp, hour_method='seasonal', holiday_method='separate')\n",
    "\n",
    "    for method, all_models_quant_reg in quant_reg_models.items():\n",
    "        \n",
    "        print(f\"method = {method}\")\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_quant_reg_direct_fcast = pd.DataFrame(index=df_fcast_dummy.index)\n",
    "        df_quant_reg_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction for Quantile Regression\n",
    "        for name, model in sorted(all_models_quant_reg.items()):\n",
    "            pred = model.predict(df_fcast_dummy.drop('timestamp_CET', axis=1))\n",
    "            df_quant_reg_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_quant_reg_direct_fcast\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # Grad Boosting\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    for method, all_models_grad_boost in grad_boost_models.items():\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "\n",
    "        if \"fturs\" in method:\n",
    "            df_fcast_dummy = data_prepro.create_features_df(df_temp, holiday_method='separate')\n",
    "            \n",
    "        elif \"dummy\" in method:\n",
    "            df_fcast_dummy = data_prepro.create_dummy_df(df_temp, hour_method='seasonal', holiday_method='separate')\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_grad_boost_direct_fcast = pd.DataFrame(index=df_fcast_dummy.index)\n",
    "        df_grad_boost_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction for Quantile Regression\n",
    "        for name, model in sorted(all_models_grad_boost.items()):\n",
    "            pred = model.predict(df_fcast_dummy.drop('timestamp_CET', axis=1))\n",
    "            df_grad_boost_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_grad_boost_direct_fcast\n",
    "        \n",
    "    # = = = = = = = = = = = = = \n",
    "    # LightGBM\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    start_date = \"2022-01-01\"\n",
    "    X_train_dummy, y_train_dummy, X_train_fturs, y_train_fturs = preprocess_data(df_energy_current, start_date)\n",
    "\n",
    "    methods = [f\"lightgbm_dummy_{i}\" for i in range(len(all_lgbm_params))]\n",
    "    \n",
    "    for method_idx, method in enumerate(methods):\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "        params = all_lgbm_params[method_idx]\n",
    "\n",
    "        if \"fturs\" in method:\n",
    "            all_models = model_train.fit_lightgbm(X_train_fturs, y_train_fturs, quantiles, params)\n",
    "            df_fcast_dummy = data_prepro.create_features_df(df_temp, holiday_method='separate')\n",
    "            \n",
    "        elif \"dummy\" in method:\n",
    "            all_models = model_train.fit_lightgbm(X_train_dummy, y_train_dummy, quantiles, params)\n",
    "            df_fcast_dummy = data_prepro.create_dummy_df(df_temp, hour_method='seasonal', holiday_method='separate')\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_direct_fcast = pd.DataFrame(index=df_fcast_dummy.index)\n",
    "        df_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction\n",
    "        for name, model in sorted(all_models.items()):\n",
    "            pred = model.predict(df_fcast_dummy.drop('timestamp_CET', axis=1))\n",
    "            df_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_direct_fcast\n",
    "        \n",
    "    # = = = = = = = = = = = = = \n",
    "    # XGBoost model\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    start_date = \"2022-01-01\"\n",
    "    X_train_dummy, y_train_dummy, X_train_fturs, y_train_fturs = preprocess_data(df_energy_current, start_date)\n",
    "    \n",
    "    df_fcast_dummy = data_prepro.create_dummy_df(df_temp, hour_method='seasonal', holiday_method='separate')\n",
    "    \n",
    "    methods = [f\"xgboost_dummy_{i}\" for i in range(len(all_xgb_params))]\n",
    "    \n",
    "    for method_idx, method in enumerate(methods):\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "        params = all_xgb_params[method_idx]\n",
    "    \n",
    "        all_models = model_train.fit_xgboost(X_train_dummy, y_train_dummy, quantiles, params)\n",
    "        dict_weekly_models[method] = all_models\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_direct_fcast = pd.DataFrame(index=df_fcast_dummy.index)\n",
    "        df_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction\n",
    "        for name, model in sorted(all_models.items()):\n",
    "            pred = model.predict(df_fcast_dummy.drop('timestamp_CET', axis=1))\n",
    "            df_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_direct_fcast\n",
    "    \n",
    "    # = = = = = = = = = = = = = \n",
    "    # Evaluation based on submission timestamps\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    # get actual values at every submission timestamp\n",
    "    df_energy_eval = df_energy.loc[df_energy['timestamp_CET'].isin(subm_timestamps)].copy()\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for model_name, forecast_df in dict_weekly_fcasts.items():\n",
    "\n",
    "        # Initialize an empty DataFrame to store quantile scores\n",
    "        quantile_scores = pd.DataFrame(index=subm_timestamps, columns=[f\"q {q:.3f}\" for q in quantiles])\n",
    "        # take subset of fcast df at submission timestamps\n",
    "        forecast_df = forecast_df.loc[forecast_df['timestamp_CET'].isin(subm_timestamps)].copy()\n",
    "\n",
    "        # Iterate over each submission timestamp\n",
    "        for q_idx, q in enumerate(quantiles):\n",
    "\n",
    "            qscore = mean_pinball_loss(alpha=q, \n",
    "                                       y_true=df_energy_eval['gesamt'].values, \n",
    "                                       y_pred=forecast_df.iloc[:,q_idx+1].values) # skip timestamp_CET col\n",
    "            \n",
    "            quantile_scores.iloc[:,q_idx] = qscore / 1000\n",
    "        \n",
    "        # Store the quantile scores for the model\n",
    "        evaluation_results[model_name] = quantile_scores\n",
    "    \n",
    "    # Calculate mean scores for each quantile over time\n",
    "    mean_scores = {}\n",
    "    for model_name, quantile_scores in evaluation_results.items():\n",
    "        mean_scores[model_name] = quantile_scores.mean()\n",
    "    \n",
    "    display(pd.DataFrame(mean_scores).T.iloc[13:].style.highlight_min(color='lightgreen', axis=0))\n",
    "\n",
    "    # calculate mean scores over all quantiles\n",
    "    mean_scores_df = pd.DataFrame(mean_scores)\n",
    "    \n",
    "    print('- '*15)\n",
    "    print('scores:')\n",
    "    print(mean_scores_df.mean(axis=0).sort_values(ascending=True))\n",
    "        \n",
    "    # = = = = = = = = = = = = = \n",
    "    # Save all fcasts & trained models for the week\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    dict_all_fcasts[fcast_date] = dict_weekly_fcasts\n",
    "    dict_all_evals[fcast_date] = evaluation_results\n",
    "    \n",
    "    with open(f'{fcast_date}_models.pickle', 'wb') as handle:\n",
    "        pickle.dump(dict_weekly_models, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'eval.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_all_evals, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'fcasts.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_all_fcasts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "os.chdir(cwd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
