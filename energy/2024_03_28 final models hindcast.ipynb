{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\statsforecast\\core.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsforecast.models import MSTL\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "\n",
    "os.chdir(\"C:/2023_11-PTSFC\")\n",
    "import model_train as model_train\n",
    "import data_prepro as data_prepro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"1\"  # Replace \"4\" with the desired number of cores\n",
    "\n",
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "fcast_hor = [36, 40, 44, 60, 64, 68] # in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 80136 entries, 2014-12-31 23:00:00+00:00 to 2024-02-21 22:00:00+00:00\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   timestamp_CET  80136 non-null  datetime64[ns, CET]\n",
      " 1   gesamt         80136 non-null  float64            \n",
      "dtypes: datetime64[ns, CET](1), float64(1)\n",
      "memory usage: 1.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = \n",
    "# get data\n",
    "# df_energy = data_prepro.get_energy_data_today(to_date=t_wednesday.strftime('%Y%m%d'))\n",
    "\n",
    "# Read data from file with specified data types\n",
    "df_energy = pd.read_csv(\"data/2015-01-01_2024-02-21_energy.csv\", index_col=0, parse_dates=[0])\n",
    "df_energy['timestamp_CET'] = pd.to_datetime(df_energy['timestamp_CET'], utc=True).dt.tz_convert('CET')\n",
    "print(df_energy.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_energy, start_date):\n",
    "    \n",
    "    df_energy_small = df_energy.loc[(df_energy['timestamp_CET'] > start_date)].copy()\n",
    "    \n",
    "    df_energy_dummy = data_prepro.create_dummy_df(df_energy_small, hour_method='simple', holiday_method='separate')\n",
    "    df_energy_fturs = data_prepro.create_features_df(df_energy_small, holiday_method='separate')\n",
    "\n",
    "    X_train_fturs = df_energy_fturs.drop(['gesamt', 'timestamp_CET'], axis=1)\n",
    "    y_train_fturs = df_energy_fturs['gesamt']\n",
    "\n",
    "    X_train_dummy = df_energy_dummy.drop(['gesamt', 'timestamp_CET'], axis=1)\n",
    "    y_train_dummy = df_energy_dummy['gesamt']\n",
    "    \n",
    "    return X_train_dummy, y_train_dummy, X_train_fturs, y_train_fturs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator DummyRegressor from version 1.3.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.3.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ytl_c\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator GradientBoostingRegressor from version 1.3.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "# import trained models\n",
    "path = \"custom_models\"\n",
    "\n",
    "def load_pickled_object(path, filename):\n",
    "    with open(f\"{path}/{filename}\", \"rb\") as handle:\n",
    "        return pickle.load(handle)\n",
    "    \n",
    "quant_reg_models = {}\n",
    "quant_reg_models['quant_reg_2015_sk'] = load_pickled_object(path, \"20150101_20231031_quant_reg_sk.pickle\")\n",
    "quant_reg_models['quant_reg_2018_sk'] = load_pickled_object(path, \"20180101_20231031_quant_reg_sk.pickle\")\n",
    "quant_reg_models['quant_reg_2015_sm'] = load_pickled_object(path, \"20150101_20231031_quant_reg_sm.pickle\")\n",
    "quant_reg_models['quant_reg_2018_sm'] = load_pickled_object(path, \"20180101_20231031_quant_reg_sm.pickle\")\n",
    "\n",
    "grad_boost_models = {}\n",
    "grad_boost_models['grad_boost_2015_fturs'] = load_pickled_object(path, \"20150101_20231031_grad_boost_fturs.pickle\")\n",
    "grad_boost_models['grad_boost_2018_fturs'] = load_pickled_object(path, \"20180101_20231031_grad_boost_fturs.pickle\")\n",
    "grad_boost_models['grad_boost_2015_dummy'] = load_pickled_object(path, \"20150101_20231031_grad_boost_dummy.pickle\")\n",
    "grad_boost_models['grad_boost_2018_dummy'] = load_pickled_object(path, \"20180101_20231031_grad_boost_dummy.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "Forecasting for week starting from 2023-11-15 ...\n",
      "Submission timestamps = 2023-11-17 12:00:00+01:00 to 2023-11-18 20:00:00+01:00\n",
      "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "method = bench_pm_2weeks\n",
      "method = bench_same_month\n",
      "method = bench_pm_1month\n",
      "method = mstl_4\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "folder_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = os.path.join(cwd, folder_name)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "os.chdir(directory)\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = pd.Timestamp('2023-11-15')\n",
    "end_date = pd.Timestamp('2024-02-14')\n",
    "\n",
    "# Generate a list of weekly dates in UTC\n",
    "fcast_dates_cet = pd.date_range(start=start_date, end=end_date, freq='W-WED').tz_localize('CET').strftime('%Y-%m-%d').tolist()\n",
    "dict_all_fcasts = {}\n",
    "dict_all_evals = {}\n",
    "\n",
    "# Iterate over the forecast dates\n",
    "for fcast_date in fcast_dates_cet[:]:\n",
    "\n",
    "    print('= '*30)\n",
    "    print(f\"Forecasting for week starting from {fcast_date} ...\")\n",
    "    \n",
    "    dict_weekly_fcasts = {}\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # generate prediction timestamps based on t0 = following thursday 00:00\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    # Calculate the Thursday and Wednesday of the week\n",
    "    t_wednesday = pd.Timestamp(fcast_date).replace(hour=0, minute=0, second=0, microsecond=0).tz_localize('CET')\n",
    "    t_thursday = t_wednesday + pd.Timedelta(days=1)\n",
    "\n",
    "    # Generate required submission timestamps\n",
    "    subm_timestamps = [(t_thursday + pd.Timedelta(hours=fcast)) for fcast in fcast_hor]\n",
    "    print(f\"Submission timestamps = {subm_timestamps[0]} to {subm_timestamps[-1]}\")\n",
    "    \n",
    "    # Create df with Information at fcast start date\n",
    "    t_thursday_str = t_thursday.strftime('%Y-%m-%d')\n",
    "    df_energy_current = df_energy.loc[df_energy['timestamp_CET'] <= t_thursday_str].copy()\n",
    "    print('= '*30)\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # Data Prep for All Methods\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    # create fcast index for next 68 hours\n",
    "    fcast_timestamp_CET = pd.date_range(start=t_thursday, periods=68+1, freq='H')\n",
    "    fcast_timestamp_UTC = fcast_timestamp_CET.tz_convert('UTC')\n",
    "\n",
    "    # create df with fcast timestamps as INPUT for model\n",
    "    df_temp = pd.DataFrame(index=fcast_timestamp_UTC)\n",
    "    df_temp['timestamp_CET'] = fcast_timestamp_CET\n",
    "    df_fcast_dummy = data_prepro.create_dummy_df(df_temp, hour_method='simple', holiday_method='separate')\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # Simple Benchmark\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    # create new dataframe with relevant info for benchmark\n",
    "    df_energy_benchmark = df_energy_current.copy()\n",
    "    df_energy_benchmark[\"month\"] = df_energy_benchmark['timestamp_CET'].dt.month\n",
    "    df_energy_benchmark[\"weekday\"] = df_energy_benchmark['timestamp_CET'].dt.weekday # Monday=0, Sunday=6\n",
    "    df_energy_benchmark[\"weeknum\"] = df_energy_benchmark['timestamp_CET'].dt.isocalendar().week\n",
    "\n",
    "    last_t = 150 # if there are more matches only take the most recent\n",
    "    pred_baseline = np.zeros((3,len(fcast_timestamp_CET),5)) # 3 condition types, 5 quantiles\n",
    "\n",
    "    for i,d in enumerate(fcast_timestamp_CET):\n",
    "            \n",
    "        weekday = d.weekday()\n",
    "        hour = d.hour\n",
    "        weeknum = d.week\n",
    "        \n",
    "        # basic condition that the weekday and hour match\n",
    "        basic_cond = (df_energy_benchmark.weekday == weekday) & (df_energy_benchmark.index.time == d.time())\n",
    "        \n",
    "        # AND the weeknum is within +/- 2 weeks of the target\n",
    "        cond1 = (df_energy_benchmark['weeknum'].between(weeknum-2, weeknum+2)) \n",
    "        # AND the month also matches\n",
    "        cond2 = (df_energy_benchmark.index.month == d.month)\n",
    "        # AND the month is within +/- 1 months of the target\n",
    "        cond3 = (df_energy_benchmark['month'].between(d.month-1, d.month+1))\n",
    "\n",
    "        cond_list = [cond1, cond2, cond3]\n",
    "\n",
    "        for cond_idx, cond in enumerate(cond_list):\n",
    "            cond = basic_cond & cond\n",
    "            match_df = df_energy_benchmark[cond]\n",
    "            # print(f\"condition {cond_idx} ... {len(match_df)} matches found\")\n",
    "            pred_baseline[cond_idx, i, :] = np.quantile(match_df.iloc[-last_t:][\"gesamt\"], q=quantiles) # method='linear'\n",
    "\n",
    "    methods = ['bench_pm_2weeks', 'bench_same_month', 'bench_pm_1month']\n",
    "    for m_idx, method in enumerate(methods):\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "        # create empty df\n",
    "        df_benchmark = pd.DataFrame(index=fcast_timestamp_UTC, columns=[f\"q {q:.3f}\" for q in quantiles])\n",
    "        df_benchmark.loc[:,:] = pred_baseline[m_idx,:,:]\n",
    "\n",
    "        # make sure all cols are float\n",
    "        df_benchmark = df_benchmark.astype(float)\n",
    "        # add CET col\n",
    "        df_benchmark['timestamp_CET'] = df_benchmark.index.tz_convert('CET')\n",
    "        # reorder cols\n",
    "        df_benchmark = df_benchmark[['timestamp_CET', 'q 0.025', 'q 0.250', 'q 0.500', 'q 0.750', 'q 0.975']]\n",
    "        # save to dict\n",
    "        dict_weekly_fcasts[method] = df_benchmark\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # MSTL\n",
    "    # = = = = = = = = = = = = = \n",
    "        \n",
    "    for mstl_train_horizon in [4, 0.5, 0.25]:\n",
    "        \n",
    "        method = f\"mstl_{mstl_train_horizon}\"\n",
    "        print(f\"method = {method}\")\n",
    "\n",
    "        df_mstl_train = df_energy_current.iloc[-int(mstl_train_horizon * 365 * 24):].copy()\n",
    "        mstl_model = MSTL(season_length=[24, 24 * 7]).fit(df_mstl_train[\"gesamt\"])\n",
    "\n",
    "        n_steps = df_benchmark.shape[0]\n",
    "\n",
    "        y_hat_dict = mstl_model.predict(h=n_steps, level=[50, 95])\n",
    "        y_hat_df = pd.DataFrame(y_hat_dict)\n",
    "        y_hat_df[\"timestamp_CET\"] = pd.date_range(start=t_thursday, periods=len(y_hat_df), freq=\"H\")\n",
    "\n",
    "        # rename columns\n",
    "        y_hat_df = y_hat_df.rename(\n",
    "            columns={\n",
    "                \"mean\": \"q 0.500\",\n",
    "                \"lo-50\": \"q 0.250\",\n",
    "                \"hi-50\": \"q 0.750\",\n",
    "                \"lo-95\": \"q 0.025\",\n",
    "                \"hi-95\": \"q 0.975\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # rearrange cols\n",
    "        y_hat_df = y_hat_df[[\"timestamp_CET\", \"q 0.025\", \"q 0.250\", \"q 0.500\", \"q 0.750\", \"q 0.975\"]]\n",
    "\n",
    "        df_mstl_fcast = y_hat_df\n",
    "        df_mstl_fcast.index = fcast_timestamp_UTC\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_mstl_fcast\n",
    "    \n",
    "    # = = = = = = = = = = = = = \n",
    "    # Quantile Reg\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    for method, all_models_quant_reg in quant_reg_models.items():\n",
    "        \n",
    "        print(f\"method = {method}\")\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_quant_reg_direct_fcast = pd.DataFrame(index=df_fcast_dummy.index)\n",
    "        df_quant_reg_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction for Quantile Regression\n",
    "        for name, model in sorted(all_models_quant_reg.items()):\n",
    "            pred = model.predict(df_fcast_dummy.drop('timestamp_CET', axis=1))\n",
    "            df_quant_reg_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_quant_reg_direct_fcast\n",
    "\n",
    "    # = = = = = = = = = = = = = \n",
    "    # Grad Boosting\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    for method, all_models_grad_boost in grad_boost_models.items():\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "\n",
    "        if \"fturs\" in method:\n",
    "            df_fcast_temp = data_prepro.create_features_df(df_temp, holiday_method='separate')\n",
    "            \n",
    "        elif \"dummy\" in method:\n",
    "            df_fcast_temp = data_prepro.create_dummy_df(df_temp, hour_method='simple', holiday_method='separate')\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_grad_boost_direct_fcast = pd.DataFrame(index=df_fcast_temp.index)\n",
    "        df_grad_boost_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction for Quantile Regression\n",
    "        for name, model in sorted(all_models_grad_boost.items()):\n",
    "            pred = model.predict(df_fcast_temp.drop('timestamp_CET', axis=1))\n",
    "            df_grad_boost_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_grad_boost_direct_fcast\n",
    "        \n",
    "    # = = = = = = = = = = = = = \n",
    "    # LightGBM\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    years = [2019, 2020, 2021, 2022]\n",
    "    methods = [f\"lightgbm_dummy_{yr}\" for yr in years]\n",
    "    \n",
    "    for method_idx, method in enumerate(methods):\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "        start_date = f\"{years[method_idx]}-01-01\"\n",
    "        X_train_dummy, y_train_dummy, X_train_fturs, y_train_fturs = preprocess_data(df_energy_current, start_date)\n",
    "\n",
    "        all_models = model_train.fit_lightgbm(X_train_dummy, y_train_dummy, quantiles)\n",
    "        df_fcast_temp = data_prepro.create_dummy_df(df_temp, hour_method='simple', holiday_method='separate')\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_direct_fcast = pd.DataFrame(index=df_fcast_temp.index)\n",
    "        df_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction\n",
    "        for name, model in sorted(all_models.items()):\n",
    "            pred = model.predict(df_fcast_temp.drop('timestamp_CET', axis=1))\n",
    "            df_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_direct_fcast\n",
    "        \n",
    "    # = = = = = = = = = = = = = \n",
    "    # XGBoost model\n",
    "    # = = = = = = = = = = = = = \n",
    "        \n",
    "    years = [2019, 2020, 2021, 2022]\n",
    "    methods = [f\"xgboost_dummy_{yr}\" for yr in years]\n",
    "\n",
    "    for method_idx, method in enumerate(methods):\n",
    "\n",
    "        print(f\"method = {method}\")\n",
    "        start_date = f\"{years[method_idx]}-01-01\"\n",
    "        X_train_dummy, y_train_dummy, X_train_fturs, y_train_fturs = preprocess_data(df_energy_current, start_date)\n",
    "    \n",
    "        all_models = model_train.fit_xgboost(X_train_dummy, y_train_dummy, quantiles)\n",
    "        df_fcast_dummy = data_prepro.create_dummy_df(df_temp, hour_method='simple', holiday_method='separate')\n",
    "\n",
    "        # create empty OUTPUT df with columns = quantiles\n",
    "        df_direct_fcast = pd.DataFrame(index=df_fcast_dummy.index)\n",
    "        df_direct_fcast['timestamp_CET'] = fcast_timestamp_CET\n",
    "\n",
    "        # Prediction\n",
    "        for name, model in sorted(all_models.items()):\n",
    "            pred = model.predict(df_fcast_dummy.drop('timestamp_CET', axis=1))\n",
    "            df_direct_fcast[name] = pred\n",
    "\n",
    "        dict_weekly_fcasts[method] = df_direct_fcast\n",
    "    \n",
    "    # = = = = = = = = = = = = = \n",
    "    # Evaluation based on submission timestamps\n",
    "    # = = = = = = = = = = = = = \n",
    "\n",
    "    # get actual values at every submission timestamp\n",
    "    df_energy_eval = df_energy.loc[df_energy['timestamp_CET'].isin(subm_timestamps)].copy()\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for model_name, forecast_df in dict_weekly_fcasts.items():\n",
    "\n",
    "        # Initialize an empty DataFrame to store quantile scores\n",
    "        quantile_scores = pd.DataFrame(index=subm_timestamps, columns=[f\"q {q:.3f}\" for q in quantiles])\n",
    "        # take subset of fcast df at submission timestamps\n",
    "        forecast_df = forecast_df.loc[forecast_df['timestamp_CET'].isin(subm_timestamps)].copy()\n",
    "\n",
    "        # Iterate over each submission timestamp\n",
    "        for q_idx, q in enumerate(quantiles):\n",
    "\n",
    "            qscore = mean_pinball_loss(alpha=q, \n",
    "                                       y_true=df_energy_eval['gesamt'].values, \n",
    "                                       y_pred=forecast_df.iloc[:,q_idx+1].values) # skip timestamp_CET col\n",
    "            \n",
    "            quantile_scores.iloc[:,q_idx] = qscore / 1000\n",
    "        \n",
    "        # Store the quantile scores for the model\n",
    "        evaluation_results[model_name] = quantile_scores\n",
    "    \n",
    "    # Calculate mean scores for each quantile over time\n",
    "    mean_scores = {}\n",
    "    for model_name, quantile_scores in evaluation_results.items():\n",
    "        mean_scores[model_name] = quantile_scores.mean()\n",
    "    \n",
    "    # df = pd.DataFrame(mean_scores).T\n",
    "    # filtered_df = df[df.index.str.contains('lightgbm')]\n",
    "    # display(filtered_df.style.highlight_min(color='lightgreen', axis=0))\n",
    "\n",
    "    # df = pd.DataFrame(mean_scores).T\n",
    "    # filtered_df = df[df.index.str.contains('xgboost')]\n",
    "    # display(filtered_df.style.highlight_min(color='lightgreen', axis=0))\n",
    "\n",
    "    # calculate mean scores over all quantiles\n",
    "    mean_scores_df = pd.DataFrame(mean_scores)\n",
    "    \n",
    "    print('- '*15)\n",
    "    print('scores:')\n",
    "    print(mean_scores_df.mean(axis=0).sort_values(ascending=True))\n",
    "        \n",
    "    # = = = = = = = = = = = = = \n",
    "    # Save all fcasts & trained models for the week\n",
    "    # = = = = = = = = = = = = = \n",
    "    \n",
    "    dict_all_fcasts[fcast_date] = dict_weekly_fcasts\n",
    "    dict_all_evals[fcast_date] = evaluation_results\n",
    "    \n",
    "with open('eval.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_all_evals, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('fcasts.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_all_fcasts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "os.chdir(cwd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
